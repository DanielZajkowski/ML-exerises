{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part g: RNN DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to classify sentiment on IMDB data\n",
    "For this exercise, we will train a \"vanilla\" RNN to predict the sentiment on IMDB reviews.  Our data consists of 25000 training sequences and 25000 test sequences.  The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n",
    "\n",
    "Keras provides a convenient interface to load the data and immediately encode the words into integers (based on the most common words).  This will save us a lot of the drudgery that is usually involved when working with raw text.\n",
    "\n",
    "We will walk through the preparation of the data and the building of an RNN model.  Then it will be your turn to build your own models (and prepare the data how you see fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n                         ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py\", line 343, in load_dynamic\n    return _load(spec)\n           ^^^^^^^^^^^\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/__init__.py:49\u001b[0m\n\u001b[1;32m     48\u001b[0m sys\u001b[39m.\u001b[39msetdlopenflags(_default_dlopen_flags \u001b[39m|\u001b[39m ctypes\u001b[39m.\u001b[39mRTLD_GLOBAL)\n\u001b[0;32m---> 49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     50\u001b[0m sys\u001b[39m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py:28\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m _mod\n\u001b[0;32m---> 28\u001b[0m _pywrap_tensorflow \u001b[39m=\u001b[39m swig_import_helper()\n\u001b[1;32m     29\u001b[0m \u001b[39mdel\u001b[39;00m swig_import_helper\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py:24\u001b[0m, in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     _mod \u001b[39m=\u001b[39m imp\u001b[39m.\u001b[39;49mload_module(\u001b[39m'\u001b[39;49m\u001b[39m_pywrap_tensorflow\u001b[39;49m\u001b[39m'\u001b[39;49m, fp, pathname, description)\n\u001b[1;32m     25\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py:243\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[39mreturn\u001b[39;00m load_dynamic(name, filename, file)\n\u001b[1;32m    244\u001b[0m \u001b[39melif\u001b[39;00m type_ \u001b[39m==\u001b[39m PKG_DIRECTORY:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py:343\u001b[0m, in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m spec \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mModuleSpec(\n\u001b[1;32m    342\u001b[0m     name\u001b[39m=\u001b[39mname, loader\u001b[39m=\u001b[39mloader, origin\u001b[39m=\u001b[39mpath)\n\u001b[0;32m--> 343\u001b[0m \u001b[39mreturn\u001b[39;00m _load(spec)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#from tensorflow import keras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#from tensorflow.keras.preprocessing import sequence\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#from tensorflow.keras.models import Sequential\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#from tensorflow.keras.datasets import imdb\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#from tensorflow.keras import initializers\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m sequence\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/engine/functional.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m     23\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39m# Lazily import the `tf.contrib` module. This avoids loading all of the\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# dependencies of `tf.contrib` at `import tensorflow` time.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_LazyContribLoader\u001b[39;00m(\u001b[39mobject\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/__init__.py:60\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m   msg \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mError importing tensorflow.  Unless you are using bazel,\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[39myou should not try to import tensorflow from its source directory;\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[39mplease exit the tensorflow source tree, and relaunch your python interpreter\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39mfrom there.\u001b[39m\u001b[39m\"\"\"\u001b[39m \u001b[39m%\u001b[39m traceback\u001b[39m.\u001b[39mformat_exc()\n\u001b[0;32m---> 60\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Protocol buffers\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgraph_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n                         ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/imp.py\", line 343, in load_dynamic\n    return _load(spec)\n           ^^^^^^^^^^^\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there."
     ]
    }
   ],
   "source": [
    "#from tensorflow import keras\n",
    "#from tensorflow.keras.preprocessing import sequence\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Embedding\n",
    "#from tensorflow.keras.layers import SimpleRNN\n",
    "#from tensorflow.keras.datasets import imdb\n",
    "#from tensorflow.keras import initializers\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 30  # maximum length of a sequence - truncate after this\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "## Load in the data.  The function automatically tokenizes the text into distinct integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 30)\n",
      "x_test shape: (25000, 30)\n"
     ]
    }
   ],
   "source": [
    "# This pads (or truncates) the sequences so that they are of the maximum length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n",
       "        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n",
       "          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n",
       "          62,   358,    99])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[123,:]  #Here's what an example sequence looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers for (Vanilla) RNNs\n",
    "\n",
    "In this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n",
    "\n",
    "### Embedding Layer\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
    "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
    "- The `input_dim` should be the size of the vocabulary.\n",
    "- The `input_length` specifies the length of the sequences that the network expects.\n",
    "\n",
    "### SimpleRNN Layer\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
    "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
    "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build a RNN\n",
    "\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 5)                 280       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,000,286\n",
      "Trainable params: 1,000,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Note that most of the parameters come from the embedding layer\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6706 - accuracy: 0.5857 - val_loss: 0.6299 - val_accuracy: 0.6816\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.5907 - accuracy: 0.7100 - val_loss: 0.5737 - val_accuracy: 0.7118\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.5228 - accuracy: 0.7493 - val_loss: 0.5284 - val_accuracy: 0.7355\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.4744 - accuracy: 0.7763 - val_loss: 0.5026 - val_accuracy: 0.7533\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.4416 - accuracy: 0.7963 - val_loss: 0.4831 - val_accuracy: 0.7652\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.4176 - accuracy: 0.8108 - val_loss: 0.4715 - val_accuracy: 0.7726\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.3992 - accuracy: 0.8217 - val_loss: 0.4627 - val_accuracy: 0.7783\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3850 - accuracy: 0.8286 - val_loss: 0.4569 - val_accuracy: 0.7828\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.3737 - accuracy: 0.8337 - val_loss: 0.4530 - val_accuracy: 0.7850\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.3648 - accuracy: 0.8369 - val_loss: 0.4510 - val_accuracy: 0.7875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27080a051c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 127us/step\n",
      "Test score: 0.4510296483039856\n",
      "Test accuracy: 0.7875199913978577\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, we will illustrate:\n",
    "- Preparing the data to use sequences of length 80 rather than length 30.  Does it improve the performance?\n",
    "- Trying different values of the \"max_features\".  Does this  improve the performance?\n",
    "- Trying smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.6337 - accuracy: 0.6565 - val_loss: 0.5743 - val_accuracy: 0.7344\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.4866 - accuracy: 0.7780 - val_loss: 0.5044 - val_accuracy: 0.7408\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.4099 - accuracy: 0.8179 - val_loss: 0.4731 - val_accuracy: 0.7641\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.3701 - accuracy: 0.8379 - val_loss: 0.4171 - val_accuracy: 0.8082\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.3402 - accuracy: 0.8560 - val_loss: 0.3996 - val_accuracy: 0.8189\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 68s 3ms/step - loss: 0.3190 - accuracy: 0.8654 - val_loss: 0.4009 - val_accuracy: 0.8171\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.3015 - accuracy: 0.8738 - val_loss: 0.3933 - val_accuracy: 0.8222\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 73s 3ms/step - loss: 0.2866 - accuracy: 0.8797 - val_loss: 0.3827 - val_accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.2748 - accuracy: 0.8858 - val_loss: 0.3739 - val_accuracy: 0.8357\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 71s 3ms/step - loss: 0.2631 - accuracy: 0.8916 - val_loss: 0.4062 - val_accuracy: 0.8227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27099d3a808>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\n",
    "maxlen = 80  # maximum length of a sequence - truncate after this\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 20\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.6718 - accuracy: 0.5425 - val_loss: 0.6276 - val_accuracy: 0.6543\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.5967 - accuracy: 0.7342 - val_loss: 0.5971 - val_accuracy: 0.7211\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.5605 - accuracy: 0.7888 - val_loss: 0.5658 - val_accuracy: 0.7946\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.5377 - accuracy: 0.8089 - val_loss: 0.5595 - val_accuracy: 0.7972\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.5206 - accuracy: 0.8184 - val_loss: 0.5430 - val_accuracy: 0.7989\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.5059 - accuracy: 0.8263 - val_loss: 0.5306 - val_accuracy: 0.8012\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4931 - accuracy: 0.8291 - val_loss: 0.5240 - val_accuracy: 0.8007\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.4809 - accuracy: 0.8335 - val_loss: 0.5187 - val_accuracy: 0.8008\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.4696 - accuracy: 0.8364 - val_loss: 0.5039 - val_accuracy: 0.8008\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.4584 - accuracy: 0.8398 - val_loss: 0.4966 - val_accuracy: 0.8006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2709db3f688>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.4484 - accuracy: 0.8431 - val_loss: 0.4879 - val_accuracy: 0.8017\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 22s 885us/step - loss: 0.4384 - accuracy: 0.8447 - val_loss: 0.4796 - val_accuracy: 0.8061\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 12s 472us/step - loss: 0.4272 - accuracy: 0.8480 - val_loss: 0.4703 - val_accuracy: 0.8071\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 13s 515us/step - loss: 0.4172 - accuracy: 0.8513 - val_loss: 0.4676 - val_accuracy: 0.8094\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 12s 472us/step - loss: 0.4085 - accuracy: 0.8535 - val_loss: 0.4590 - val_accuracy: 0.8118\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 12s 483us/step - loss: 0.4003 - accuracy: 0.8558 - val_loss: 0.4546 - val_accuracy: 0.8114\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 13s 519us/step - loss: 0.3919 - accuracy: 0.8564 - val_loss: 0.4476 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 12s 469us/step - loss: 0.3845 - accuracy: 0.8589 - val_loss: 0.4396 - val_accuracy: 0.8129\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 12s 472us/step - loss: 0.3778 - accuracy: 0.8621 - val_loss: 0.4336 - val_accuracy: 0.8152\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 12s 471us/step - loss: 0.3713 - accuracy: 0.8636 - val_loss: 0.4350 - val_accuracy: 0.8151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2708af2f048>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of curiosity, run for 10 more epochs\n",
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
